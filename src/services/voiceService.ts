/**
 * ğŸ”§ è¯­éŸ³äº¤äº’æœåŠ¡ - é‡æ„ç‰ˆæœ¬
 * 
 * ä¸»è¦æ”¹è¿›ï¼š
 * 1. æ¨¡å—åŒ–è®¾è®¡ï¼Œåˆ†ç¦»å…³æ³¨ç‚¹
 * 2. æ›´å¥½çš„çŠ¶æ€ç®¡ç†å’Œé”™è¯¯å¤„ç†
 * 3. æ€§èƒ½ä¼˜åŒ–å’Œç¼“å­˜æœºåˆ¶
 * 4. ç±»å‹å®‰å…¨å’Œå¯æµ‹è¯•æ€§
 */

import { generateMinimaxAudio, VoiceSettings } from './geminiService';

// ==================== ç±»å‹å®šä¹‰ ====================

export interface VoiceRecognitionState {
  isListening: boolean;
  isSupported: boolean;
  transcript: string;
  confidence: number;
  error?: string;
}

export interface VoiceSynthesisState {
  isSpeaking: boolean;
  isSupported: boolean;
  currentText?: string;
  error?: string;
}

export interface VoiceEvents {
  onRecognitionStart?: () => void;
  onRecognitionEnd?: () => void;
  onRecognitionResult?: (text: string, confidence: number) => void;
  onRecognitionError?: (error: string) => void;

  onSynthesisStart?: (text: string) => void;
  onSynthesisEnd?: () => void;
  onSynthesisError?: (error: string) => void;
}

export interface VoiceSynthesisOptions {
  lang?: string;
  rate?: number;
  pitch?: number;
  volume?: number;
  useMiniMax?: boolean;
  voiceSettings?: Partial<VoiceSettings>;
}

export interface VoiceRecognitionOptions {
  continuous?: boolean;
  interimResults?: boolean;
  lang?: string;
  maxAlternatives?: number;
}

// ==================== é”™è¯¯å¤„ç† ====================

export class VoiceError extends Error {
  constructor(
    message: string,
    public readonly code: string,
    public readonly recoverable: boolean = true
  ) {
    super(message);
    this.name = 'VoiceError';
  }
}

// ==================== çŠ¶æ€ç®¡ç†å™¨ ====================

class StateManager<T> {
  private state: T;
  private listeners: Array<(state: T) => void> = [];

  constructor(initialState: T) {
    this.state = { ...initialState };
  }

  getState(): T {
    return { ...this.state };
  }

  setState(updates: Partial<T>): void {
    this.state = { ...this.state, ...updates };
    this.notifyListeners();
  }

  subscribe(listener: (state: T) => void): () => void {
    this.listeners.push(listener);
    return () => {
      this.listeners = this.listeners.filter(l => l !== listener);
    };
  }

  private notifyListeners(): void {
    this.listeners.forEach(listener => listener(this.getState()));
  }
}

// ==================== éŸ³é¢‘ç¼“å­˜ç®¡ç† ====================

class AudioCache {
  private cache = new Map<string, string>();
  private maxSize = 10; // æœ€å¤šç¼“å­˜10ä¸ªéŸ³é¢‘

  get(text: string): string | undefined {
    return this.cache.get(text);
  }

  set(text: string, audioBase64: string): void {
    // å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
    if (this.cache.size >= this.maxSize) {
      const firstKey = this.cache.keys().next().value;
      if (firstKey) {
        this.cache.delete(firstKey);
      }
    }
    this.cache.set(text, audioBase64);
  }

  clear(): void {
    this.cache.clear();
  }

  has(text: string): boolean {
    return this.cache.has(text);
  }
}

// ==================== é‡è¯•æœºåˆ¶ ====================

class RetryManager {
  static async withRetry<T>(
    operation: () => Promise<T>,
    maxRetries: number = 3,
    delay: number = 1000
  ): Promise<T> {
    let lastError: Error;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        return await operation();
      } catch (error) {
        lastError = error instanceof Error ? error : new Error(String(error));
        
        if (attempt === maxRetries) {
          throw lastError;
        }

        // æŒ‡æ•°é€€é¿
        const waitTime = delay * Math.pow(2, attempt - 1);
        await new Promise(resolve => setTimeout(resolve, waitTime));
      }
    }

    throw lastError!;
  }
}

// ==================== è¯­éŸ³è¯†åˆ«æ¨¡å— ====================

class VoiceRecognitionManager {
  private recognition: SpeechRecognition | null = null;
  private stateManager: StateManager<VoiceRecognitionState>;
  private events: VoiceEvents;

  constructor(stateManager: StateManager<VoiceRecognitionState>, events: VoiceEvents) {
    this.stateManager = stateManager;
    this.events = events;
  }

  async initialize(): Promise<void> {
    try {
      // æ£€æŸ¥æµè§ˆå™¨æ”¯æŒ
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      
      if (!SpeechRecognition) {
        throw new VoiceError(
          'æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½',
          'NOT_SUPPORTED',
          false
        );
      }

      // è¯·æ±‚éº¦å…‹é£æƒé™
      await this.requestMicrophonePermission();

      // åˆ›å»ºè¯­éŸ³è¯†åˆ«å®ä¾‹
      this.recognition = new SpeechRecognition();
      this.setupRecognitionEvents();

      this.stateManager.setState({ isSupported: true });
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'åˆå§‹åŒ–å¤±è´¥';
      this.stateManager.setState({
        isSupported: false,
        error: errorMessage
      });
      throw error;
    }
  }

  private async requestMicrophonePermission(): Promise<void> {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      stream.getTracks().forEach(track => track.stop()); // ç«‹å³åœæ­¢ï¼Œåªæ˜¯æµ‹è¯•æƒé™
    } catch (error) {
      throw new VoiceError(
        'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸éº¦å…‹é£è®¿é—®',
        'PERMISSION_DENIED',
        false
      );
    }
  }

  private setupRecognitionEvents(): void {
    if (!this.recognition) return;

    this.recognition.continuous = false;
    this.recognition.interimResults = true;
    this.recognition.lang = 'zh-CN';
    this.recognition.maxAlternatives = 1;

    this.recognition.onstart = () => {
      this.stateManager.setState({
        isListening: true,
        transcript: '',
        confidence: 0,
        error: undefined
      });
      this.events.onRecognitionStart?.();
    };

    this.recognition.onresult = (event: SpeechRecognitionEvent) => {
      let finalTranscript = '';
      let interimTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        const transcript = result[0].transcript;
        const confidence = result[0].confidence;

        if (result.isFinal) {
          finalTranscript += transcript;
          this.events.onRecognitionResult?.(transcript, confidence);
        } else {
          interimTranscript += transcript;
        }
      }

      this.stateManager.setState({
        transcript: finalTranscript || interimTranscript,
        confidence: event.results[event.resultIndex]?.[0]?.confidence || 0,
      });
    };

    this.recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      const errorMessage = this.getRecognitionErrorMessage(event.error);
      this.stateManager.setState({
        isListening: false,
        error: errorMessage
      });
      this.events.onRecognitionError?.(errorMessage);
    };

    this.recognition.onend = () => {
      this.stateManager.setState({ isListening: false });
      this.events.onRecognitionEnd?.();
    };
  }

  async start(options: VoiceRecognitionOptions = {}): Promise<void> {
    if (!this.recognition) {
      throw new VoiceError('è¯­éŸ³è¯†åˆ«æœªåˆå§‹åŒ–', 'NOT_INITIALIZED');
    }

    const currentState = this.stateManager.getState();
    if (!currentState.isSupported) {
      throw new VoiceError('è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨', 'NOT_SUPPORTED', false);
    }

    if (currentState.isListening) {
      console.warn('è¯­éŸ³è¯†åˆ«å·²åœ¨è¿›è¡Œä¸­');
      return;
    }

    // åº”ç”¨é€‰é¡¹
    if (options.continuous !== undefined) this.recognition.continuous = options.continuous;
    if (options.interimResults !== undefined) this.recognition.interimResults = options.interimResults;
    if (options.lang !== undefined) this.recognition.lang = options.lang;
    if (options.maxAlternatives !== undefined) this.recognition.maxAlternatives = options.maxAlternatives;

    try {
      this.recognition.start();
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'å¯åŠ¨å¤±è´¥';
      this.stateManager.setState({ error: errorMessage });
      throw error;
    }
  }

  stop(): void {
    if (this.recognition && this.stateManager.getState().isListening) {
      this.recognition.stop();
    }
  }

  dispose(): void {
    if (this.recognition) {
      this.recognition.stop();
      this.recognition = null;
    }
  }

  private getRecognitionErrorMessage(error: SpeechRecognitionErrorCode): string {
    const errorMessages: Record<string, string> = {
      'no-speech': 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•',
      aborted: 'è¯­éŸ³è¯†åˆ«è¢«ä¸­æ–­',
      'audio-capture': 'éŸ³é¢‘æ•è·å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£',
      network: 'ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ',
      'not-allowed': 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸',
      'service-not-allowed': 'è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨',
      'bad-grammar': 'è¯­éŸ³è¯†åˆ«è¯­æ³•é”™è¯¯',
      'language-not-supported': 'ä¸æ”¯æŒçš„è¯­è¨€è®¾ç½®',
    };

    return errorMessages[error] || 'è¯­éŸ³è¯†åˆ«å‘ç”ŸæœªçŸ¥é”™è¯¯';
  }
}

// ==================== è¯­éŸ³åˆæˆæ¨¡å— ====================

class VoiceSynthesisManager {
  private synthesis: SpeechSynthesis | null = null;
  private stateManager: StateManager<VoiceSynthesisState>;
  private events: VoiceEvents;
  private audioCache: AudioCache;

  constructor(
    stateManager: StateManager<VoiceSynthesisState>,
    events: VoiceEvents,
    audioCache: AudioCache
  ) {
    this.stateManager = stateManager;
    this.events = events;
    this.audioCache = audioCache;
  }

  initialize(): void {
    if (!('speechSynthesis' in window)) {
      this.stateManager.setState({
        isSupported: false,
        error: 'æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³åˆæˆåŠŸèƒ½'
      });
      return;
    }

    this.synthesis = window.speechSynthesis;
    this.stateManager.setState({ isSupported: true });
  }

  async synthesize(
    text: string,
    options: VoiceSynthesisOptions = {}
  ): Promise<void> {
    const currentState = this.stateManager.getState();
    if (!currentState.isSupported) {
      throw new VoiceError('è¯­éŸ³åˆæˆä¸å¯ç”¨', 'NOT_SUPPORTED', false);
    }

    // åœæ­¢å½“å‰æ’­æ”¾
    if (currentState.isSpeaking) {
      this.stop();
    }

    // ä¼˜å…ˆä½¿ç”¨MiniMax APIï¼ˆå¦‚æœå¯ç”¨ä¸”æœªç¦ç”¨ï¼‰
    if (options.useMiniMax !== false) {
      try {
        await this.synthesizeWithMiniMax(text, options.voiceSettings);
        return;
      } catch (error) {
        console.warn('MiniMaxè¯­éŸ³åˆæˆå¤±è´¥ï¼Œé™çº§åˆ°æµè§ˆå™¨TTS:', error);
        // é™çº§åˆ°æµè§ˆå™¨TTS
      }
    }

    // ä½¿ç”¨æµè§ˆå™¨å†…ç½®TTS
    await this.synthesizeWithBrowser(text, options);
  }

  private async synthesizeWithMiniMax(
    text: string,
    voiceSettings?: Partial<VoiceSettings>
  ): Promise<void> {
    this.stateManager.setState({
      isSpeaking: true,
      currentText: text,
      error: undefined
    });
    this.events.onSynthesisStart?.(text);

    try {
      // æ£€æŸ¥ç¼“å­˜
      let audioBase64 = this.audioCache.get(text);
      
      if (!audioBase64) {
        // ä½¿ç”¨é‡è¯•æœºåˆ¶è°ƒç”¨API
        audioBase64 = await RetryManager.withRetry(
          () => generateMinimaxAudio(text, voiceSettings),
          2, // æœ€å¤šé‡è¯•2æ¬¡
          1000
        );

        if (!audioBase64) {
          throw new VoiceError('MiniMax APIè¿”å›ç©ºéŸ³é¢‘', 'API_ERROR');
        }

        // ç¼“å­˜éŸ³é¢‘
        this.audioCache.set(text, audioBase64);
      }

      // æ’­æ”¾éŸ³é¢‘
      await this.playBase64Audio(audioBase64);

      this.stateManager.setState({
        isSpeaking: false,
        currentText: undefined
      });
      this.events.onSynthesisEnd?.();
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'è¯­éŸ³åˆæˆå¤±è´¥';
      this.stateManager.setState({
        isSpeaking: false,
        error: errorMessage
      });
      this.events.onSynthesisError?.(errorMessage);
      throw error;
    }
  }

  private async synthesizeWithBrowser(
    text: string,
    options: VoiceSynthesisOptions
  ): Promise<void> {
    if (!this.synthesis) {
      throw new VoiceError('è¯­éŸ³åˆæˆæœªåˆå§‹åŒ–', 'NOT_INITIALIZED');
    }

    return new Promise((resolve, reject) => {
      const utterance = new SpeechSynthesisUtterance(text);

      utterance.lang = options.lang || 'zh-CN';
      utterance.rate = options.rate || 0.9;
      utterance.pitch = options.pitch || 1;
      utterance.volume = options.volume || 1;

      utterance.onstart = () => {
        this.stateManager.setState({
          isSpeaking: true,
          currentText: text,
          error: undefined
        });
        this.events.onSynthesisStart?.(text);
      };

      utterance.onend = () => {
        this.stateManager.setState({
          isSpeaking: false,
          currentText: undefined
        });
        this.events.onSynthesisEnd?.();
        resolve();
      };

      utterance.onerror = (event) => {
        const errorMessage = `è¯­éŸ³åˆæˆå¤±è´¥: ${event.error}`;
        this.stateManager.setState({
          isSpeaking: false,
          error: errorMessage
        });
        this.events.onSynthesisError?.(errorMessage);
        reject(new VoiceError(errorMessage, 'SYNTHESIS_ERROR'));
      };

      this.synthesis!.speak(utterance);
    });
  }

  private async playBase64Audio(audioBase64: string): Promise<void> {
    return new Promise((resolve, reject) => {
      try {
        // ä¼˜åŒ–çš„Base64è§£ç 
        const binaryString = atob(audioBase64);
        const bytes = new Uint8Array(binaryString.length);
        
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }

        const blob = new Blob([bytes], { type: 'audio/mp3' });
        const audioUrl = URL.createObjectURL(blob);

        const audio = new Audio(audioUrl);

        audio.onended = () => {
          URL.revokeObjectURL(audioUrl);
          resolve();
        };

        audio.onerror = () => {
          URL.revokeObjectURL(audioUrl);
          reject(new VoiceError('éŸ³é¢‘æ’­æ”¾å¤±è´¥', 'PLAYBACK_ERROR'));
        };

        audio.play().catch(reject);
      } catch (error) {
        reject(new VoiceError('éŸ³é¢‘å¤„ç†å¤±è´¥', 'AUDIO_PROCESSING_ERROR'));
      }
    });
  }

  stop(): void {
    if (this.synthesis) {
      this.synthesis.cancel();
      this.stateManager.setState({
        isSpeaking: false,
        currentText: undefined
      });
    }
  }

  dispose(): void {
    this.stop();
    this.synthesis = null;
  }
}

// ==================== ä¸»è¯­éŸ³æœåŠ¡ç±» ====================

export class VoiceService {
  private recognitionManager: VoiceRecognitionManager;
  private synthesisManager: VoiceSynthesisManager;
  private recognitionStateManager: StateManager<VoiceRecognitionState>;
  private synthesisStateManager: StateManager<VoiceSynthesisState>;
  private audioCache: AudioCache;
  private events: VoiceEvents;
  private isInitialized: boolean = false;

  constructor(events: VoiceEvents = {}) {
    this.events = events;
    
    // åˆå§‹åŒ–çŠ¶æ€ç®¡ç†å™¨
    this.recognitionStateManager = new StateManager<VoiceRecognitionState>({
      isListening: false,
      isSupported: false,
      transcript: '',
      confidence: 0,
    });

    this.synthesisStateManager = new StateManager<VoiceSynthesisState>({
      isSpeaking: false,
      isSupported: false,
    });

    // åˆå§‹åŒ–éŸ³é¢‘ç¼“å­˜
    this.audioCache = new AudioCache();

    // åˆå§‹åŒ–ç®¡ç†å™¨
    this.recognitionManager = new VoiceRecognitionManager(
      this.recognitionStateManager,
      this.events
    );

    this.synthesisManager = new VoiceSynthesisManager(
      this.synthesisStateManager,
      this.events,
      this.audioCache
    );

    // å¼‚æ­¥åˆå§‹åŒ–
    this.initialize();
  }

  private async initialize(): Promise<void> {
    try {
      console.log('ğŸ¤ åˆå§‹åŒ–è¯­éŸ³æœåŠ¡...');

      // å¹¶è¡Œåˆå§‹åŒ–è¯†åˆ«å’Œåˆæˆ
      await Promise.all([
        this.recognitionManager.initialize(),
        Promise.resolve(this.synthesisManager.initialize())
      ]);

      this.isInitialized = true;
      console.log('âœ… è¯­éŸ³æœåŠ¡åˆå§‹åŒ–å®Œæˆ', {
        recognitionSupported: this.recognitionStateManager.getState().isSupported,
        synthesisSupported: this.synthesisStateManager.getState().isSupported,
      });
    } catch (error) {
      console.error('âŒ è¯­éŸ³æœåŠ¡åˆå§‹åŒ–å¤±è´¥:', error);
    }
  }

  // è¯­éŸ³è¯†åˆ«æ–¹æ³•
  async startRecognition(options?: VoiceRecognitionOptions): Promise<void> {
    if (!this.isInitialized) {
      await this.initialize();
    }
    return this.recognitionManager.start(options);
  }

  stopRecognition(): void {
    this.recognitionManager.stop();
  }

  // è¯­éŸ³åˆæˆæ–¹æ³•
  async synthesize(text: string, options?: VoiceSynthesisOptions): Promise<void> {
    if (!this.isInitialized) {
      await this.initialize();
    }
    return this.synthesisManager.synthesize(text, options);
  }

  stopSynthesis(): void {
    this.synthesisManager.stop();
  }

  // çŠ¶æ€è·å–æ–¹æ³•
  getRecognitionState(): VoiceRecognitionState {
    return this.recognitionStateManager.getState();
  }

  getSynthesisState(): VoiceSynthesisState {
    return this.synthesisStateManager.getState();
  }

  // çŠ¶æ€è®¢é˜…æ–¹æ³•
  subscribeToRecognitionState(listener: (state: VoiceRecognitionState) => void): () => void {
    return this.recognitionStateManager.subscribe(listener);
  }

  subscribeToSynthesisState(listener: (state: VoiceSynthesisState) => void): () => void {
    return this.synthesisStateManager.subscribe(listener);
  }

  // æ”¯æŒæ£€æŸ¥
  isRecognitionSupported(): boolean {
    return this.recognitionStateManager.getState().isSupported;
  }

  isSynthesisSupported(): boolean {
    return this.synthesisStateManager.getState().isSupported;
  }

  // ç¼“å­˜ç®¡ç†
  clearAudioCache(): void {
    this.audioCache.clear();
  }

  // æ¸…ç†èµ„æº
  dispose(): void {
    this.recognitionManager.dispose();
    this.synthesisManager.dispose();
    this.audioCache.clear();
    console.log('ğŸ§¹ è¯­éŸ³æœåŠ¡å·²æ¸…ç†');
  }
}

// ==================== å…¨å±€å®ä¾‹ç®¡ç† ====================

class VoiceServiceManager {
  private instance: VoiceService | null = null;

  getInstance(events?: VoiceEvents): VoiceService {
    if (!this.instance) {
      this.instance = new VoiceService(events);
    }
    return this.instance;
  }

  dispose(): void {
    if (this.instance) {
      this.instance.dispose();
      this.instance = null;
    }
  }
}

const voiceServiceManager = new VoiceServiceManager();

// ==================== å¯¼å‡ºå‡½æ•° ====================

export const getVoiceService = (events?: VoiceEvents): VoiceService => {
  return voiceServiceManager.getInstance(events);
};

export const startVoiceRecognition = async (
  events?: VoiceEvents,
  options?: VoiceRecognitionOptions
): Promise<void> => {
  const service = getVoiceService(events);
  return service.startRecognition(options);
};

export const stopVoiceRecognition = (): void => {
  const service = voiceServiceManager.getInstance();
  service.stopRecognition();
};

export const speakText = async (
  text: string,
  options: VoiceSynthesisOptions = {}
): Promise<void> => {
  const service = getVoiceService();
  return service.synthesize(text, options);
};

export const disposeVoiceService = (): void => {
  voiceServiceManager.dispose();
};

// ==================== React Hook ====================

import { useState, useEffect, useCallback, useRef } from 'react';

export const useVoiceService = (events?: VoiceEvents) => {
  const serviceRef = useRef<VoiceService | null>(null);
  const [recognitionState, setRecognitionState] = useState<VoiceRecognitionState>({
    isListening: false,
    isSupported: false,
    transcript: '',
    confidence: 0,
  });
  const [synthesisState, setSynthesisState] = useState<VoiceSynthesisState>({
    isSpeaking: false,
    isSupported: false,
  });

  useEffect(() => {
    const service = getVoiceService(events);
    serviceRef.current = service;

    // è®¢é˜…çŠ¶æ€å˜æ›´
    const unsubscribeRecognition = service.subscribeToRecognitionState(setRecognitionState);
    const unsubscribeSynthesis = service.subscribeToSynthesisState(setSynthesisState);

    return () => {
      unsubscribeRecognition();
      unsubscribeSynthesis();
    };
  }, [events]);

  const startRecognition = useCallback(
    (options?: VoiceRecognitionOptions) => {
      return serviceRef.current?.startRecognition(options);
    },
    []
  );

  const stopRecognition = useCallback(() => {
    serviceRef.current?.stopRecognition();
  }, []);

  const speakText = useCallback(
    (text: string, options?: VoiceSynthesisOptions) => {
      return serviceRef.current?.synthesize(text, options);
    },
    []
  );

  const stopSynthesis = useCallback(() => {
    serviceRef.current?.stopSynthesis();
  }, []);

  const clearCache = useCallback(() => {
    serviceRef.current?.clearAudioCache();
  }, []);

  return {
    recognitionState,
    synthesisState,
    startRecognition,
    stopRecognition,
    speakText,
    stopSynthesis,
    clearCache,
    isRecognitionSupported: recognitionState.isSupported,
    isSynthesisSupported: synthesisState.isSupported,
  };
};

export default VoiceService;